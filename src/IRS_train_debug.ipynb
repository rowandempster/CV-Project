{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import time\n",
    "from typing import Callable, Optional\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from pathlib import Path\n",
    "\n",
    "from IRS.dataloader.IRSLoader import IRSDataset\n",
    "from IRS.utils.AverageMeter import AverageMeter\n",
    "from IRS.utils.common import logger\n",
    "from IRS.losses.multiscaleloss import EPE\n",
    "from IRS.dataloader.IRSLoader import IRSDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from IRS.networks.DispNetC import DispNetC\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.models as models\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class DisparityTrainer(object):\n",
    "    def __init__(\n",
    "        self,\n",
    "        lr: float,\n",
    "        device: str,\n",
    "        trainlist: str,\n",
    "        vallist: str,\n",
    "        datapath: str,\n",
    "        batch_size: int,\n",
    "        maxdisp: int,\n",
    "        criterion: Callable,\n",
    "        pretrain: Optional[str]=None,\n",
    "        num_workers=4\n",
    "    ):\n",
    "        super(DisparityTrainer, self).__init__()\n",
    "        self.lr = lr\n",
    "        self.current_lr = lr\n",
    "        self.device = device\n",
    "        self.trainlist = trainlist\n",
    "        self.vallist = vallist\n",
    "        self.datapath = datapath\n",
    "        self.batch_size = batch_size\n",
    "        self.pretrain = pretrain\n",
    "        self.maxdisp = maxdisp\n",
    "        self.num_workers = num_workers\n",
    "        self.criterion = criterion\n",
    "        self.epe = EPE\n",
    "\n",
    "        self.initialize()\n",
    "\n",
    "    def _prepare_dataset(self):\n",
    "        train_dataset = IRSDataset(txt_file=self.trainlist, root_dir=self.datapath, phase='train', load_norm=False)\n",
    "        test_dataset = IRSDataset(txt_file=self.vallist, root_dir=self.datapath, phase='test', load_norm=False)\n",
    "        self.img_size = train_dataset.get_img_size()\n",
    "        self.scale_size = train_dataset.get_scale_size()\n",
    "        self.focal_length = train_dataset.get_focal_length()\n",
    "\n",
    "        self.train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "\n",
    "        self.test_loader = DataLoader(\n",
    "            test_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "        self.num_batches_per_epoch = len(self.train_loader)\n",
    "\n",
    "    def _build_net(self):\n",
    "        self.net = DispNetC(batchNorm=False, input_channel=3, maxdisp=30).to(self.device)\n",
    "\n",
    "        if self.pretrain is not None:\n",
    "            assert Path(self.pretrain).exists(), f\"{self.pretrain} does not exist\"\n",
    "            model_data = torch.load(self.pretrain)\n",
    "            logger.info(\"Load pretrain model: %s\", self.pretrain)\n",
    "            if \"state_dict\" in model_data.keys():\n",
    "                self.net.load_state_dict(model_data[\"state_dict\"])\n",
    "            elif \"model\" in model_data.keys():\n",
    "                self.net.load_state_dict(model_data[\"model\"])\n",
    "            else:\n",
    "                self.net.load_state_dict(model_data)\n",
    "\n",
    "    def _build_optimizer(self):\n",
    "        beta = 0.999\n",
    "        momentum = 0.9\n",
    "        self.optimizer = torch.optim.Adam(\n",
    "            filter(lambda p: p.requires_grad, self.net.parameters()),\n",
    "            self.lr,\n",
    "            betas=(momentum, beta),\n",
    "            amsgrad=True,\n",
    "        )\n",
    "\n",
    "    def initialize(self):\n",
    "        self._prepare_dataset()\n",
    "        self._build_net()\n",
    "        self._build_optimizer()\n",
    "\n",
    "    def adjust_learning_rate(self, epoch):\n",
    "        cur_lr = self.lr / (2 ** (epoch // 10))\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group[\"lr\"] = cur_lr\n",
    "        self.current_lr = cur_lr\n",
    "        return cur_lr\n",
    "\n",
    "    def train_one_epoch(self, epoch):\n",
    "        batch_time = AverageMeter()\n",
    "        data_time = AverageMeter()\n",
    "        losses = AverageMeter()\n",
    "        EPEs = AverageMeter()\n",
    "        # switch to train mode\n",
    "        self.net.train()\n",
    "        end = time.time()\n",
    "        cur_lr = self.adjust_learning_rate(epoch)\n",
    "        logger.info(\"learning rate of epoch %d: %f.\" % (epoch, cur_lr))\n",
    "\n",
    "        for i_batch, sample_batched in enumerate(self.train_loader):\n",
    "\n",
    "            left_input = sample_batched[\"img_left\"].to(self.device)\n",
    "            right_input = sample_batched[\"img_right\"].to(self.device)\n",
    "            input = torch.cat((left_input, right_input), 1)\n",
    "            target_disp = sample_batched[\"gt_disp\"].to(self.device)\n",
    "            data_time.update(time.time() - end)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            disps = self.net(input)\n",
    "            loss = self.criterion(left_input, right_input, disps[0])\n",
    "            # loss = nn.L1Loss()(disps[0], target_disp)\n",
    "            epe = self.epe(disps[0], target_disp)\n",
    "            # record loss and EPE\n",
    "            losses.update(loss.data.item(), input.size(0))\n",
    "            EPEs.update(epe.data.item(), input.size(0))\n",
    "            # compute gradient and do SGD step\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            if i_batch % 10 == 0:\n",
    "                logger.info(\n",
    "                    \"Epoch: [{0}][{1}/{2}]\\t\"\n",
    "                    \"Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t\"\n",
    "                    \"Data {data_time.val:.3f} ({data_time.avg:.3f})\\t\"\n",
    "                    \"Loss {loss.val:.3f} ({loss.avg:.3f})\\t\"\n",
    "                    \"EPE {EPE.val:.3f} ({EPE.avg:.3f})\\t\".format(\n",
    "                        epoch,\n",
    "                        i_batch,\n",
    "                        self.num_batches_per_epoch,\n",
    "                        batch_time=batch_time,\n",
    "                        data_time=data_time,\n",
    "                        loss=losses,\n",
    "                        EPE=EPEs,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        return losses.avg, EPEs.avg\n",
    "\n",
    "    def validate(self):\n",
    "        batch_time = AverageMeter()\n",
    "        EPEs = AverageMeter()\n",
    "        # switch to evaluate mode\n",
    "        end = time.time()\n",
    "        self.net.eval()\n",
    "        for i, sample_batched in enumerate(self.test_loader):\n",
    "\n",
    "            left_input = sample_batched[\"img_left\"].to(self.device)\n",
    "            right_input = sample_batched[\"img_right\"].to(self.device)\n",
    "            left_input = F.interpolate(left_input, self.scale_size, mode=\"bilinear\")\n",
    "            right_input = F.interpolate(right_input, self.scale_size, mode=\"bilinear\")\n",
    "\n",
    "            input = torch.cat((left_input, right_input), 1)\n",
    "\n",
    "            target_disp = sample_batched[\"gt_disp\"].to(self.device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                disp = self.net(input)[0]\n",
    "\n",
    "            # upsampling the predicted disparity map\n",
    "            disp = nn.Upsample(size=target_disp.shape[2:], mode='bilinear')(disp)\n",
    "            epe = self.epe(disp, target_disp)\n",
    "\n",
    "            # record loss and EPE\n",
    "            EPEs.update(epe.data.item(), input.size(0))\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            logger.info(\n",
    "                \"Test: [{0}/{1}]\\t Time {2}\\t EPE {3}\".format(\n",
    "                    i,\n",
    "                    len(self.test_loader),\n",
    "                    batch_time.val,\n",
    "                    EPEs.val,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        logger.info(\" * EPE {:.3f}\".format(EPEs.avg))\n",
    "        return EPEs.avg\n",
    "\n",
    "    def get_model(self):\n",
    "        return self.net.state_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_left(im_l: torch.Tensor, im_r: torch.Tensor, disp: torch.Tensor):\n",
    "    '''\n",
    "    im_l, im_r: [N, C(3), H, W]\n",
    "    disp: [N, C(1), H, W]\n",
    "    '''\n",
    "    N, _, H, W = disp.shape\n",
    "    x_base = torch.linspace(0, W-1, W).repeat(N, 1, H, 1).to(im_r.device)\n",
    "    # [1, H, W]\n",
    "    x_query = (x_base - disp).round().long()\n",
    "    valid_mask = x_query >= 0\n",
    "    return torch.where(valid_mask, im_r.gather(3, x_query.clip(0, 959).expand_as(im_r)), im_l)\n",
    "\n",
    "class PerceptualLoss(nn.Module):\n",
    "    L1 = 4\n",
    "    L2 = 9\n",
    "    L3 = 16\n",
    "    L4 = 30\n",
    "\n",
    "    def __init__(self, layers: List[int], device: str):\n",
    "        super(PerceptualLoss, self).__init__()\n",
    "        self.layers = layers\n",
    "        vgg16 = models.vgg16(pretrained=True)\n",
    "        self.feat_seq = list(vgg16.named_children())[0][1].to(device)\n",
    "\n",
    "    def _compute_l1_diff(self, inputs: torch.Tensor, targets: torch.Tensor):\n",
    "        '''\n",
    "        Returns [N, M, H, W] where M is len(self.layers)\n",
    "        '''\n",
    "        diffs = []\n",
    "        H, W = inputs.shape[2:]\n",
    "        up = torch.nn.Upsample(size=(H,W))\n",
    "        for layer in self.layers:\n",
    "            input_map = self.feat_seq[0:layer](inputs)\n",
    "            target_map = self.feat_seq[0:layer](targets)\n",
    "            diffs.append(up(torch.norm(input_map - target_map, p=1, dim=1).unsqueeze(1)).squeeze(1))\n",
    "        return torch.stack(diffs, dim=1)\n",
    "\n",
    "    def _compare_images(self, est: torch.Tensor, gt: torch.Tensor):\n",
    "        '''\n",
    "        est, gt: [N, C(3), H, W]\n",
    "        '''\n",
    "        return self._compute_l1_diff(est, gt)\n",
    "\n",
    "    def forward(self, im_l: torch.Tensor, im_r: torch.Tensor, disp: torch.Tensor):\n",
    "        '''\n",
    "        im_l, im_r: [N, C(3), H, W]\n",
    "        disp: [N, C(1), H, W]\n",
    "        '''\n",
    "        est_l = estimate_left(im_l, im_r, disp)\n",
    "        return est_l, self._compare_images(est_l, im_l)\n",
    "\n",
    "class L1Loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(L1Loss, self).__init__()\n",
    "    \n",
    "    def forward(self, im_l: torch.Tensor, im_r: torch.Tensor, disp: torch.Tensor):\n",
    "        est_l = estimate_left(im_l, im_r, disp)\n",
    "        return nn.L1Loss()(im_l, im_l)\n",
    "\n",
    "class SSLCriterion(nn.Module):\n",
    "    def __init__(self, modules: nn.ModuleList):\n",
    "        super(SSLCriterion, self).__init__()\n",
    "        self.modules = modules\n",
    "\n",
    "    def forward(self, im_l: torch.Tensor, im_r: torch.Tensor, disp: torch.Tensor):\n",
    "        '''\n",
    "        im_l, im_r: [N, C(3), H, W]\n",
    "        disp: [N, C(1), H, W]\n",
    "        '''\n",
    "        return sum([mod(im_l, im_r, disp) for mod in self.modules])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/project/IRS/networks/DispNetC.py:117: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  kaiming_normal(m.weight.data)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'per_loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/project/IRS_train_debug.ipynb Cell 3'\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f757365725f6465765f31222c2273657474696e6773223a7b22686f7374223a227373683a2f2f747270726f2d7562756e7475312e7761746f636c75737465722e6c6f63616c227d7d/project/IRS_train_debug.ipynb#ch0000002vscode-remote?line=14'>15</a>\u001b[0m im_l \u001b[39m=\u001b[39m batch[\u001b[39m'\u001b[39m\u001b[39mimg_left\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto(DEVICE)\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f757365725f6465765f31222c2273657474696e6773223a7b22686f7374223a227373683a2f2f747270726f2d7562756e7475312e7761746f636c75737465722e6c6f63616c227d7d/project/IRS_train_debug.ipynb#ch0000002vscode-remote?line=15'>16</a>\u001b[0m im_r \u001b[39m=\u001b[39m batch[\u001b[39m'\u001b[39m\u001b[39mimg_right\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto(DEVICE)\n\u001b[0;32m---> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f757365725f6465765f31222c2273657474696e6773223a7b22686f7374223a227373683a2f2f747270726f2d7562756e7475312e7761746f636c75737465722e6c6f63616c227d7d/project/IRS_train_debug.ipynb#ch0000002vscode-remote?line=16'>17</a>\u001b[0m est_l, loss_l \u001b[39m=\u001b[39m per_loss(im_l, im_r, batch[\u001b[39m'\u001b[39m\u001b[39mgt_disp\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto(DEVICE))\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f757365725f6465765f31222c2273657474696e6773223a7b22686f7374223a227373683a2f2f747270726f2d7562756e7475312e7761746f636c75737465722e6c6f63616c227d7d/project/IRS_train_debug.ipynb#ch0000002vscode-remote?line=18'>19</a>\u001b[0m fig \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39mfigure(figsize\u001b[39m=\u001b[39m(\u001b[39m20\u001b[39m,\u001b[39m20\u001b[39m))\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f757365725f6465765f31222c2273657474696e6773223a7b22686f7374223a227373683a2f2f747270726f2d7562756e7475312e7761746f636c75737465722e6c6f63616c227d7d/project/IRS_train_debug.ipynb#ch0000002vscode-remote?line=19'>20</a>\u001b[0m ax1 \u001b[39m=\u001b[39m fig\u001b[39m.\u001b[39madd_subplot(\u001b[39m5\u001b[39m,\u001b[39m2\u001b[39m,\u001b[39m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'per_loss' is not defined"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "DEVICE = 'cuda:0'\n",
    "DATA_DIR = \"/data\"\n",
    "NEW_LIST = 'IRS/lists/IRS_restaurant_metal_test.list'\n",
    "BATCH_SIZE = 2\n",
    "NUM_WORKERS = 1\n",
    "dataset = IRSDataset(txt_file=NEW_LIST, root_dir=DATA_DIR, phase='visualize')\n",
    "train_loader = DataLoader(dataset, batch_size = BATCH_SIZE, \\\n",
    "                                shuffle = True, num_workers = NUM_WORKERS, \\\n",
    "                                pin_memory = True)\n",
    "net = DispNetC(batchNorm=False, input_channel=3, maxdisp=30).to(DEVICE)\n",
    "batch = next(iter(train_loader))\n",
    "# per_loss = PerceptualLoss([PerceptualLoss.L1], DEVICE)\n",
    "l1_loss = L1Loss()\n",
    "im_l = batch['img_left'].to(DEVICE)\n",
    "im_r = batch['img_right'].to(DEVICE)\n",
    "est_l, loss_l = per_loss(im_l, im_r, batch['gt_disp'].to(DEVICE))\n",
    "\n",
    "fig = plt.figure(figsize=(20,20))\n",
    "ax1 = fig.add_subplot(5,2,1)\n",
    "plt.title('left')\n",
    "ax1.imshow(im_l[0].cpu().moveaxis(0, -1))\n",
    "ax2 = fig.add_subplot(5,2,2)\n",
    "plt.title('right')\n",
    "ax2.imshow(im_r[0].cpu().moveaxis(0, -1))\n",
    "ax3 = fig.add_subplot(5,2,3)\n",
    "plt.title('est_l')\n",
    "ax3.imshow(est_l[0].cpu().moveaxis(0, -1))\n",
    "ax3 = fig.add_subplot(5,2,4)\n",
    "plt.title('loss_l')\n",
    "ax3.imshow(loss_l[0].cpu().moveaxis(0, -1))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-18 17:46:15,025 [2218717997.py:123] INFO learning rate of epoch 0: 0.000200.\n",
      "/project/IRS/utils/preprocess.py:154: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)\n",
      "  gs[0].mul_(0.299).add_(0.587, gs[1]).add_(0.114, gs[2])\n",
      "/project/IRS/utils/preprocess.py:154: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)\n",
      "  gs[0].mul_(0.299).add_(0.587, gs[1]).add_(0.114, gs[2])\n",
      "/project/IRS/utils/preprocess.py:154: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)\n",
      "  gs[0].mul_(0.299).add_(0.587, gs[1]).add_(0.114, gs[2])\n",
      "/project/IRS/utils/preprocess.py:154: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)\n",
      "  gs[0].mul_(0.299).add_(0.587, gs[1]).add_(0.114, gs[2])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'int' and 'tuple'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/project/IRS_train_debug.ipynb Cell 4'\u001b[0m in \u001b[0;36m<cell line: 29>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f757365725f6465765f31222c2273657474696e6773223a7b22686f7374223a227373683a2f2f747270726f2d7562756e7475312e7761746f636c75737465722e6c6f63616c227d7d/project/IRS_train_debug.ipynb#ch0000003vscode-remote?line=15'>16</a>\u001b[0m trainer \u001b[39m=\u001b[39m DisparityTrainer(\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f757365725f6465765f31222c2273657474696e6773223a7b22686f7374223a227373683a2f2f747270726f2d7562756e7475312e7761746f636c75737465722e6c6f63616c227d7d/project/IRS_train_debug.ipynb#ch0000003vscode-remote?line=16'>17</a>\u001b[0m     lr\u001b[39m=\u001b[39mLR,\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f757365725f6465765f31222c2273657474696e6773223a7b22686f7374223a227373683a2f2f747270726f2d7562756e7475312e7761746f636c75737465722e6c6f63616c227d7d/project/IRS_train_debug.ipynb#ch0000003vscode-remote?line=17'>18</a>\u001b[0m     device\u001b[39m=\u001b[39mDEVICE,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f757365725f6465765f31222c2273657474696e6773223a7b22686f7374223a227373683a2f2f747270726f2d7562756e7475312e7761746f636c75737465722e6c6f63616c227d7d/project/IRS_train_debug.ipynb#ch0000003vscode-remote?line=25'>26</a>\u001b[0m     num_workers\u001b[39m=\u001b[39mNUM_WORKERS\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f757365725f6465765f31222c2273657474696e6773223a7b22686f7374223a227373683a2f2f747270726f2d7562756e7475312e7761746f636c75737465722e6c6f63616c227d7d/project/IRS_train_debug.ipynb#ch0000003vscode-remote?line=26'>27</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f757365725f6465765f31222c2273657474696e6773223a7b22686f7374223a227373683a2f2f747270726f2d7562756e7475312e7761746f636c75737465722e6c6f63616c227d7d/project/IRS_train_debug.ipynb#ch0000003vscode-remote?line=28'>29</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(EPOCHS):\n\u001b[0;32m---> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f757365725f6465765f31222c2273657474696e6773223a7b22686f7374223a227373683a2f2f747270726f2d7562756e7475312e7761746f636c75737465722e6c6f63616c227d7d/project/IRS_train_debug.ipynb#ch0000003vscode-remote?line=29'>30</a>\u001b[0m     trainer\u001b[39m.\u001b[39;49mtrain_one_epoch(epoch)\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f757365725f6465765f31222c2273657474696e6773223a7b22686f7374223a227373683a2f2f747270726f2d7562756e7475312e7761746f636c75737465722e6c6f63616c227d7d/project/IRS_train_debug.ipynb#ch0000003vscode-remote?line=30'>31</a>\u001b[0m     trainer\u001b[39m.\u001b[39mvalidate()\n",
      "\u001b[1;32m/project/IRS_train_debug.ipynb Cell 1'\u001b[0m in \u001b[0;36mDisparityTrainer.train_one_epoch\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f757365725f6465765f31222c2273657474696e6773223a7b22686f7374223a227373683a2f2f747270726f2d7562756e7475312e7761746f636c75737465722e6c6f63616c227d7d/project/IRS_train_debug.ipynb#ch0000000vscode-remote?line=132'>133</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m    <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f757365725f6465765f31222c2273657474696e6773223a7b22686f7374223a227373683a2f2f747270726f2d7562756e7475312e7761746f636c75737465722e6c6f63616c227d7d/project/IRS_train_debug.ipynb#ch0000000vscode-remote?line=134'>135</a>\u001b[0m disps \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnet(\u001b[39minput\u001b[39m)\n\u001b[0;32m--> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f757365725f6465765f31222c2273657474696e6773223a7b22686f7374223a227373683a2f2f747270726f2d7562756e7475312e7761746f636c75737465722e6c6f63616c227d7d/project/IRS_train_debug.ipynb#ch0000000vscode-remote?line=135'>136</a>\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcriterion(left_input, right_input, disps[\u001b[39m0\u001b[39;49m])\n\u001b[1;32m    <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f757365725f6465765f31222c2273657474696e6773223a7b22686f7374223a227373683a2f2f747270726f2d7562756e7475312e7761746f636c75737465722e6c6f63616c227d7d/project/IRS_train_debug.ipynb#ch0000000vscode-remote?line=136'>137</a>\u001b[0m \u001b[39m# loss = nn.L1Loss()(disps[0], target_disp)\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f757365725f6465765f31222c2273657474696e6773223a7b22686f7374223a227373683a2f2f747270726f2d7562756e7475312e7761746f636c75737465722e6c6f63616c227d7d/project/IRS_train_debug.ipynb#ch0000000vscode-remote?line=137'>138</a>\u001b[0m epe \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepe(disps[\u001b[39m0\u001b[39m], target_disp)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py?line=1097'>1098</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py?line=1098'>1099</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py?line=1099'>1100</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py?line=1100'>1101</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py?line=1101'>1102</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py?line=1102'>1103</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py?line=1103'>1104</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/project/IRS_train_debug.ipynb Cell 2'\u001b[0m in \u001b[0;36mSSLCriterion.forward\u001b[0;34m(self, im_l, im_r, disp)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f757365725f6465765f31222c2273657474696e6773223a7b22686f7374223a227373683a2f2f747270726f2d7562756e7475312e7761746f636c75737465722e6c6f63616c227d7d/project/IRS_train_debug.ipynb#ch0000001vscode-remote?line=64'>65</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, im_l: torch\u001b[39m.\u001b[39mTensor, im_r: torch\u001b[39m.\u001b[39mTensor, disp: torch\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f757365725f6465765f31222c2273657474696e6773223a7b22686f7374223a227373683a2f2f747270726f2d7562756e7475312e7761746f636c75737465722e6c6f63616c227d7d/project/IRS_train_debug.ipynb#ch0000001vscode-remote?line=65'>66</a>\u001b[0m     \u001b[39m'''\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f757365725f6465765f31222c2273657474696e6773223a7b22686f7374223a227373683a2f2f747270726f2d7562756e7475312e7761746f636c75737465722e6c6f63616c227d7d/project/IRS_train_debug.ipynb#ch0000001vscode-remote?line=66'>67</a>\u001b[0m \u001b[39m    im_l, im_r: [N, C(3), H, W]\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f757365725f6465765f31222c2273657474696e6773223a7b22686f7374223a227373683a2f2f747270726f2d7562756e7475312e7761746f636c75737465722e6c6f63616c227d7d/project/IRS_train_debug.ipynb#ch0000001vscode-remote?line=67'>68</a>\u001b[0m \u001b[39m    disp: [N, C(1), H, W]\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f757365725f6465765f31222c2273657474696e6773223a7b22686f7374223a227373683a2f2f747270726f2d7562756e7475312e7761746f636c75737465722e6c6f63616c227d7d/project/IRS_train_debug.ipynb#ch0000001vscode-remote?line=68'>69</a>\u001b[0m \u001b[39m    '''\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f757365725f6465765f31222c2273657474696e6773223a7b22686f7374223a227373683a2f2f747270726f2d7562756e7475312e7761746f636c75737465722e6c6f63616c227d7d/project/IRS_train_debug.ipynb#ch0000001vscode-remote?line=69'>70</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msum\u001b[39;49m([mod(im_l, im_r, disp) \u001b[39mfor\u001b[39;49;00m mod \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodules])\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'int' and 'tuple'"
     ]
    }
   ],
   "source": [
    "# Main training loop\n",
    "LR = 0.0002\n",
    "DEVICE = 'cuda:0'\n",
    "DATA_DIR = \"/data\"\n",
    "TRAIN_LIST = 'IRS/lists/Restaurant_TRAIN.list'\n",
    "TEST_METAL_LIST = 'IRS/lists/IRS_restaurant_metal_test.list'\n",
    "PRETRAIN_CHECKPOINT = None\n",
    "BATCH_SIZE = 2\n",
    "MAX_DISP = 200\n",
    "NUM_WORKERS = 4\n",
    "EPOCHS = 1\n",
    "\n",
    "l1_loss = L1Loss()\n",
    "per_loss = PerceptualLoss([PerceptualLoss.L1], DEVICE)\n",
    "criterion = SSLCriterion([per_loss])\n",
    "trainer = DisparityTrainer(\n",
    "    lr=LR,\n",
    "    device=DEVICE,\n",
    "    trainlist=TRAIN_LIST,\n",
    "    vallist=TEST_METAL_LIST,\n",
    "    datapath=DATA_DIR,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    maxdisp=MAX_DISP,\n",
    "    criterion=criterion,\n",
    "    pretrain=PRETRAIN_CHECKPOINT,\n",
    "    num_workers=NUM_WORKERS\n",
    ")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    trainer.train_one_epoch(epoch)\n",
    "    trainer.validate()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
