{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import time\n",
    "from typing import Callable, Optional\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from pathlib import Path\n",
    "\n",
    "from IRS.dataloader.IRSLoader import IRSDataset\n",
    "from IRS.utils.AverageMeter import AverageMeter\n",
    "from IRS.utils.common import logger\n",
    "from IRS.losses.multiscaleloss import EPE\n",
    "from IRS.dataloader.IRSLoader import IRSDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from IRS.networks.DispNetC import DispNetC\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.models as models\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class DisparityTrainer(object):\n",
    "    def __init__(\n",
    "        self,\n",
    "        lr: float,\n",
    "        device: str,\n",
    "        trainlist: str,\n",
    "        vallist: str,\n",
    "        datapath: str,\n",
    "        batch_size: int,\n",
    "        maxdisp: int,\n",
    "        criterion: Callable,\n",
    "        pretrain: Optional[str]=None,\n",
    "        num_workers=4\n",
    "    ):\n",
    "        super(DisparityTrainer, self).__init__()\n",
    "        self.lr = lr\n",
    "        self.current_lr = lr\n",
    "        self.device = device\n",
    "        self.trainlist = trainlist\n",
    "        self.vallist = vallist\n",
    "        self.datapath = datapath\n",
    "        self.batch_size = batch_size\n",
    "        self.pretrain = pretrain\n",
    "        self.maxdisp = maxdisp\n",
    "        self.num_workers = num_workers\n",
    "        self.criterion = criterion\n",
    "        self.epe = EPE\n",
    "\n",
    "        self.initialize()\n",
    "\n",
    "    def _prepare_dataset(self):\n",
    "        train_dataset = IRSDataset(txt_file=self.trainlist, root_dir=self.datapath, phase='train', load_norm=False)\n",
    "        test_dataset = IRSDataset(txt_file=self.vallist, root_dir=self.datapath, phase='test', load_norm=False)\n",
    "        self.img_size = train_dataset.get_img_size()\n",
    "        self.scale_size = train_dataset.get_scale_size()\n",
    "        self.focal_length = train_dataset.get_focal_length()\n",
    "\n",
    "        self.train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "\n",
    "        self.test_loader = DataLoader(\n",
    "            test_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "        self.num_batches_per_epoch = len(self.train_loader)\n",
    "\n",
    "    def _build_net(self):\n",
    "        self.net = DispNetC(batchNorm=False, input_channel=3, maxdisp=30).to(self.device)\n",
    "\n",
    "        if self.pretrain is not None:\n",
    "            assert Path(self.pretrain).exists(), f\"{self.pretrain} does not exist\"\n",
    "            model_data = torch.load(self.pretrain)\n",
    "            logger.info(\"Load pretrain model: %s\", self.pretrain)\n",
    "            if \"state_dict\" in model_data.keys():\n",
    "                self.net.load_state_dict(model_data[\"state_dict\"])\n",
    "            elif \"model\" in model_data.keys():\n",
    "                self.net.load_state_dict(model_data[\"model\"])\n",
    "            else:\n",
    "                self.net.load_state_dict(model_data)\n",
    "\n",
    "    def _build_optimizer(self):\n",
    "        beta = 0.999\n",
    "        momentum = 0.9\n",
    "        self.optimizer = torch.optim.Adam(\n",
    "            filter(lambda p: p.requires_grad, self.net.parameters()),\n",
    "            self.lr,\n",
    "            betas=(momentum, beta),\n",
    "            amsgrad=True,\n",
    "        )\n",
    "\n",
    "    def initialize(self):\n",
    "        self._prepare_dataset()\n",
    "        self._build_net()\n",
    "        self._build_optimizer()\n",
    "\n",
    "    def adjust_learning_rate(self, epoch):\n",
    "        cur_lr = self.lr / (2 ** (epoch // 10))\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group[\"lr\"] = cur_lr\n",
    "        self.current_lr = cur_lr\n",
    "        return cur_lr\n",
    "\n",
    "    def train_one_epoch(self, epoch):\n",
    "        batch_time = AverageMeter()\n",
    "        data_time = AverageMeter()\n",
    "        losses = AverageMeter()\n",
    "        EPEs = AverageMeter()\n",
    "        # switch to train mode\n",
    "        self.net.train()\n",
    "        end = time.time()\n",
    "        cur_lr = self.adjust_learning_rate(epoch)\n",
    "        logger.info(\"learning rate of epoch %d: %f.\" % (epoch, cur_lr))\n",
    "\n",
    "        for i_batch, sample_batched in enumerate(self.train_loader):\n",
    "\n",
    "            left_input = sample_batched[\"img_left\"].to(self.device)\n",
    "            right_input = sample_batched[\"img_right\"].to(self.device)\n",
    "            input = torch.cat((left_input, right_input), 1)\n",
    "            target_disp = sample_batched[\"gt_disp\"].to(self.device)\n",
    "            data_time.update(time.time() - end)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            disps = self.net(input)\n",
    "            loss = self.criterion(left_input, right_input, disps[0])\n",
    "            epe = self.epe(disps[0], target_disp)\n",
    "            # record loss and EPE\n",
    "            losses.update(loss.data.item(), input.size(0))\n",
    "            EPEs.update(epe.data.item(), input.size(0))\n",
    "            # compute gradient and do SGD step\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            if i_batch % 10 == 0:\n",
    "                logger.info(\n",
    "                    \"Epoch: [{0}][{1}/{2}]\\t\"\n",
    "                    \"Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t\"\n",
    "                    \"Data {data_time.val:.3f} ({data_time.avg:.3f})\\t\"\n",
    "                    \"Loss {loss.val:.3f} ({loss.avg:.3f})\\t\"\n",
    "                    \"EPE {EPE.val:.3f} ({EPE.avg:.3f})\\t\".format(\n",
    "                        epoch,\n",
    "                        i_batch,\n",
    "                        self.num_batches_per_epoch,\n",
    "                        batch_time=batch_time,\n",
    "                        data_time=data_time,\n",
    "                        loss=losses,\n",
    "                        EPE=EPEs,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        return losses.avg, EPEs.avg\n",
    "\n",
    "    def validate(self):\n",
    "        batch_time = AverageMeter()\n",
    "        EPEs = AverageMeter()\n",
    "        # switch to evaluate mode\n",
    "        end = time.time()\n",
    "        self.net.eval()\n",
    "        for i, sample_batched in enumerate(self.test_loader):\n",
    "\n",
    "            left_input = sample_batched[\"img_left\"].to(self.device)\n",
    "            right_input = sample_batched[\"img_right\"].to(self.device)\n",
    "            left_input = F.interpolate(left_input, self.scale_size, mode=\"bilinear\")\n",
    "            right_input = F.interpolate(right_input, self.scale_size, mode=\"bilinear\")\n",
    "\n",
    "            input = torch.cat((left_input, right_input), 1)\n",
    "\n",
    "            target_disp = sample_batched[\"gt_disp\"].to(self.device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                disp = self.net(input)[0]\n",
    "\n",
    "            # upsampling the predicted disparity map\n",
    "            disp = nn.Upsample(size=target_disp.shape[2:], mode='bilinear')(disp)\n",
    "            epe = self.epe(disp, target_disp)\n",
    "\n",
    "            # record loss and EPE\n",
    "            EPEs.update(epe.data.item(), input.size(0))\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            logger.info(\n",
    "                \"Test: [{0}/{1}]\\t Time {2}\\t EPE {3}\".format(\n",
    "                    i,\n",
    "                    len(self.test_loader),\n",
    "                    batch_time.val,\n",
    "                    EPEs.val,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        logger.info(\" * EPE {:.3f}\".format(EPEs.avg))\n",
    "        return EPEs.avg\n",
    "\n",
    "    def get_model(self):\n",
    "        return self.net.state_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_left(im_l: torch.Tensor, im_r: torch.Tensor, disp: torch.Tensor):\n",
    "    '''\n",
    "    im_l, im_r: [N, C(3), H, W]\n",
    "    disp: [N, C(1), H, W]\n",
    "    '''\n",
    "    N, _, H, W = disp.shape\n",
    "    x_base = torch.linspace(0, W-1, W).repeat(N, 1, H, 1).to(im_r.device)\n",
    "    # [1, H, W]\n",
    "    x_query = (x_base - disp).round().long()\n",
    "    valid_mask = x_query >= 0\n",
    "    return torch.where(valid_mask, im_r.gather(3, x_query.clip(0, 959).expand_as(im_r)), im_l)\n",
    "\n",
    "class PerceptualLoss(nn.Module):\n",
    "    L1 = 4\n",
    "    L2 = 9\n",
    "    L3 = 16\n",
    "    L4 = 30\n",
    "\n",
    "    def __init__(self, layers: List[int], device: str):\n",
    "        super(PerceptualLoss, self).__init__()\n",
    "        self.layers = layers\n",
    "        vgg16 = models.vgg16(pretrained=True)\n",
    "        self.feat_seq = list(vgg16.named_children())[0][1].to(device)\n",
    "\n",
    "    def _compute_l1_diff(self, inputs: torch.Tensor, targets: torch.Tensor):\n",
    "        '''\n",
    "        Returns [N, M, H, W] where M is len(self.layers)\n",
    "        '''\n",
    "        diffs = []\n",
    "        H, W = inputs.shape[2:]\n",
    "        up = torch.nn.Upsample(size=(H,W))\n",
    "        for layer in self.layers:\n",
    "            input_map = self.feat_seq[0:layer](inputs)\n",
    "            target_map = self.feat_seq[0:layer](targets)\n",
    "            diffs.append(up(torch.norm(input_map - target_map, p=1, dim=1).unsqueeze(1)).squeeze(1))\n",
    "        return torch.stack(diffs, dim=1)\n",
    "\n",
    "    def _compare_images(self, est: torch.Tensor, gt: torch.Tensor):\n",
    "        '''\n",
    "        est, gt: [N, C(3), H, W]\n",
    "        '''\n",
    "        return torch.prod(self._compute_l1_diff(est, gt), dim=1).mean()\n",
    "\n",
    "    def forward(self, im_l: torch.Tensor, im_r: torch.Tensor, disp: torch.Tensor):\n",
    "        '''\n",
    "        im_l, im_r: [N, C(3), H, W]\n",
    "        disp: [N, C(1), H, W]\n",
    "        '''\n",
    "        return self._compare_images(estimate_left(im_l, im_r, disp), im_l)\n",
    "\n",
    "class L1Loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(L1Loss, self).__init__()\n",
    "    \n",
    "    def forward(self, im_l: torch.Tensor, im_r: torch.Tensor, disp: torch.Tensor):\n",
    "        # est_l = estimate_left(im_l, im_r, disp)\n",
    "        return torch.mean(torch.abs(disp - disp))\n",
    "\n",
    "class SSLCriterion(nn.Module):\n",
    "    def __init__(self, modules: nn.ModuleList):\n",
    "        super(SSLCriterion, self).__init__()\n",
    "        self.modules = modules\n",
    "\n",
    "    def forward(self, im_l: torch.Tensor, im_r: torch.Tensor, disp: torch.Tensor):\n",
    "        '''\n",
    "        im_l, im_r: [N, C(3), H, W]\n",
    "        disp: [N, C(1), H, W]\n",
    "        '''\n",
    "        return sum([mod(im_l, im_r, disp) for mod in self.modules])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/project/IRS/networks/DispNetC.py:117: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  kaiming_normal(m.weight.data)\n",
      "2022-04-18 17:55:55,154 [1048796056.py:123] INFO learning rate of epoch 0: 0.000200.\n",
      "/project/IRS/utils/preprocess.py:154: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)\n",
      "  gs[0].mul_(0.299).add_(0.587, gs[1]).add_(0.114, gs[2])\n",
      "/project/IRS/utils/preprocess.py:154: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)\n",
      "  gs[0].mul_(0.299).add_(0.587, gs[1]).add_(0.114, gs[2])\n",
      "/project/IRS/utils/preprocess.py:154: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)\n",
      "  gs[0].mul_(0.299).add_(0.587, gs[1]).add_(0.114, gs[2])\n",
      "/project/IRS/utils/preprocess.py:154: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)\n",
      "  gs[0].mul_(0.299).add_(0.587, gs[1]).add_(0.114, gs[2])\n",
      "/usr/local/lib/python3.8/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "2022-04-18 17:55:56,807 [1048796056.py:150] INFO Epoch: [0][0/2342]\tTime 1.652 (1.652)\tData 1.299 (1.299)\tLoss 0.000 (0.000)\tEPE 13.325 (13.325)\t\n",
      "2022-04-18 17:56:00,654 [1048796056.py:150] INFO Epoch: [0][10/2342]\tTime 0.383 (0.500)\tData 0.011 (0.130)\tLoss 0.000 (0.000)\tEPE 12.174 (12.063)\t\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/project/IRS_train.ipynb Cell 3'\u001b[0m in \u001b[0;36m<cell line: 29>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f757365725f6465765f31222c2273657474696e6773223a7b22686f7374223a227373683a2f2f747270726f2d7562756e7475312e7761746f636c75737465722e6c6f63616c227d7d/project/IRS_train.ipynb#ch0000002vscode-remote?line=15'>16</a>\u001b[0m trainer \u001b[39m=\u001b[39m DisparityTrainer(\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f757365725f6465765f31222c2273657474696e6773223a7b22686f7374223a227373683a2f2f747270726f2d7562756e7475312e7761746f636c75737465722e6c6f63616c227d7d/project/IRS_train.ipynb#ch0000002vscode-remote?line=16'>17</a>\u001b[0m     lr\u001b[39m=\u001b[39mLR,\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f757365725f6465765f31222c2273657474696e6773223a7b22686f7374223a227373683a2f2f747270726f2d7562756e7475312e7761746f636c75737465722e6c6f63616c227d7d/project/IRS_train.ipynb#ch0000002vscode-remote?line=17'>18</a>\u001b[0m     device\u001b[39m=\u001b[39mDEVICE,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f757365725f6465765f31222c2273657474696e6773223a7b22686f7374223a227373683a2f2f747270726f2d7562756e7475312e7761746f636c75737465722e6c6f63616c227d7d/project/IRS_train.ipynb#ch0000002vscode-remote?line=25'>26</a>\u001b[0m     num_workers\u001b[39m=\u001b[39mNUM_WORKERS\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f757365725f6465765f31222c2273657474696e6773223a7b22686f7374223a227373683a2f2f747270726f2d7562756e7475312e7761746f636c75737465722e6c6f63616c227d7d/project/IRS_train.ipynb#ch0000002vscode-remote?line=26'>27</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f757365725f6465765f31222c2273657474696e6773223a7b22686f7374223a227373683a2f2f747270726f2d7562756e7475312e7761746f636c75737465722e6c6f63616c227d7d/project/IRS_train.ipynb#ch0000002vscode-remote?line=28'>29</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(EPOCHS):\n\u001b[0;32m---> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f757365725f6465765f31222c2273657474696e6773223a7b22686f7374223a227373683a2f2f747270726f2d7562756e7475312e7761746f636c75737465722e6c6f63616c227d7d/project/IRS_train.ipynb#ch0000002vscode-remote?line=29'>30</a>\u001b[0m     trainer\u001b[39m.\u001b[39;49mtrain_one_epoch(epoch)\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f757365725f6465765f31222c2273657474696e6773223a7b22686f7374223a227373683a2f2f747270726f2d7562756e7475312e7761746f636c75737465722e6c6f63616c227d7d/project/IRS_train.ipynb#ch0000002vscode-remote?line=30'>31</a>\u001b[0m     trainer\u001b[39m.\u001b[39mvalidate()\n",
      "\u001b[1;32m/project/IRS_train.ipynb Cell 1'\u001b[0m in \u001b[0;36mDisparityTrainer.train_one_epoch\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f757365725f6465765f31222c2273657474696e6773223a7b22686f7374223a227373683a2f2f747270726f2d7562756e7475312e7761746f636c75737465722e6c6f63616c227d7d/project/IRS_train.ipynb#ch0000000vscode-remote?line=140'>141</a>\u001b[0m \u001b[39m# compute gradient and do SGD step\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f757365725f6465765f31222c2273657474696e6773223a7b22686f7374223a227373683a2f2f747270726f2d7562756e7475312e7761746f636c75737465722e6c6f63616c227d7d/project/IRS_train.ipynb#ch0000000vscode-remote?line=141'>142</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m--> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f757365725f6465765f31222c2273657474696e6773223a7b22686f7374223a227373683a2f2f747270726f2d7562756e7475312e7761746f636c75737465722e6c6f63616c227d7d/project/IRS_train.ipynb#ch0000000vscode-remote?line=142'>143</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m    <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f757365725f6465765f31222c2273657474696e6773223a7b22686f7374223a227373683a2f2f747270726f2d7562756e7475312e7761746f636c75737465722e6c6f63616c227d7d/project/IRS_train.ipynb#ch0000000vscode-remote?line=144'>145</a>\u001b[0m \u001b[39m# measure elapsed time\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f757365725f6465765f31222c2273657474696e6773223a7b22686f7374223a227373683a2f2f747270726f2d7562756e7475312e7761746f636c75737465722e6c6f63616c227d7d/project/IRS_train.ipynb#ch0000000vscode-remote?line=145'>146</a>\u001b[0m batch_time\u001b[39m.\u001b[39mupdate(time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m end)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/optim/optimizer.py:88\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.8/dist-packages/torch/optim/optimizer.py?line=85'>86</a>\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.8/dist-packages/torch/optim/optimizer.py?line=86'>87</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m---> <a href='file:///usr/local/lib/python3.8/dist-packages/torch/optim/optimizer.py?line=87'>88</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/autograd/grad_mode.py:28\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.8/dist-packages/torch/autograd/grad_mode.py?line=24'>25</a>\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.8/dist-packages/torch/autograd/grad_mode.py?line=25'>26</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.8/dist-packages/torch/autograd/grad_mode.py?line=26'>27</a>\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m():\n\u001b[0;32m---> <a href='file:///usr/local/lib/python3.8/dist-packages/torch/autograd/grad_mode.py?line=27'>28</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/optim/adam.py:133\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/dist-packages/torch/optim/adam.py?line=129'>130</a>\u001b[0m             \u001b[39m# record the step after step update\u001b[39;00m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/dist-packages/torch/optim/adam.py?line=130'>131</a>\u001b[0m             state_steps\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m--> <a href='file:///usr/local/lib/python3.8/dist-packages/torch/optim/adam.py?line=132'>133</a>\u001b[0m     F\u001b[39m.\u001b[39;49madam(params_with_grad,\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/dist-packages/torch/optim/adam.py?line=133'>134</a>\u001b[0m            grads,\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/dist-packages/torch/optim/adam.py?line=134'>135</a>\u001b[0m            exp_avgs,\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/dist-packages/torch/optim/adam.py?line=135'>136</a>\u001b[0m            exp_avg_sqs,\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/dist-packages/torch/optim/adam.py?line=136'>137</a>\u001b[0m            max_exp_avg_sqs,\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/dist-packages/torch/optim/adam.py?line=137'>138</a>\u001b[0m            state_steps,\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/dist-packages/torch/optim/adam.py?line=138'>139</a>\u001b[0m            amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/dist-packages/torch/optim/adam.py?line=139'>140</a>\u001b[0m            beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/dist-packages/torch/optim/adam.py?line=140'>141</a>\u001b[0m            beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/dist-packages/torch/optim/adam.py?line=141'>142</a>\u001b[0m            lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/dist-packages/torch/optim/adam.py?line=142'>143</a>\u001b[0m            weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/dist-packages/torch/optim/adam.py?line=143'>144</a>\u001b[0m            eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/dist-packages/torch/optim/adam.py?line=144'>145</a>\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/optim/_functional.py:98\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.8/dist-packages/torch/optim/_functional.py?line=93'>94</a>\u001b[0m     denom \u001b[39m=\u001b[39m (exp_avg_sq\u001b[39m.\u001b[39msqrt() \u001b[39m/\u001b[39m math\u001b[39m.\u001b[39msqrt(bias_correction2))\u001b[39m.\u001b[39madd_(eps)\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.8/dist-packages/torch/optim/_functional.py?line=95'>96</a>\u001b[0m step_size \u001b[39m=\u001b[39m lr \u001b[39m/\u001b[39m bias_correction1\n\u001b[0;32m---> <a href='file:///usr/local/lib/python3.8/dist-packages/torch/optim/_functional.py?line=97'>98</a>\u001b[0m param\u001b[39m.\u001b[39;49maddcdiv_(exp_avg, denom, value\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49mstep_size)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Main training loop\n",
    "LR = 0.0002\n",
    "DEVICE = 'cuda:1'\n",
    "DATA_DIR = \"/data\"\n",
    "TRAIN_LIST = 'IRS/lists/Restaurant_TRAIN.list'\n",
    "TEST_METAL_LIST = 'IRS/lists/IRS_restaurant_metal_test.list'\n",
    "PRETRAIN_CHECKPOINT = None\n",
    "BATCH_SIZE = 8\n",
    "MAX_DISP = 200\n",
    "NUM_WORKERS = 4\n",
    "EPOCHS = 30\n",
    "\n",
    "# per_loss = PerceptualLoss([PerceptualLoss.L1], DEVICE)\n",
    "l1_loss = L1Loss()\n",
    "criterion = SSLCriterion([l1_loss])\n",
    "trainer = DisparityTrainer(\n",
    "    lr=LR,\n",
    "    device=DEVICE,\n",
    "    trainlist=TRAIN_LIST,\n",
    "    vallist=TEST_METAL_LIST,\n",
    "    datapath=DATA_DIR,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    maxdisp=MAX_DISP,\n",
    "    criterion=criterion,\n",
    "    pretrain=PRETRAIN_CHECKPOINT,\n",
    "    num_workers=NUM_WORKERS\n",
    ")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    trainer.train_one_epoch(epoch)\n",
    "    trainer.validate()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
