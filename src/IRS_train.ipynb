{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import time\n",
    "from typing import Callable, Optional\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from pathlib import Path\n",
    "\n",
    "from IRS.dataloader.IRSLoader import IRSDataset\n",
    "from IRS.utils.AverageMeter import AverageMeter\n",
    "from IRS.utils.common import logger\n",
    "from IRS.losses.multiscaleloss import EPE\n",
    "from IRS.dataloader.IRSLoader import IRSDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from IRS.networks.DispNetC import DispNetC\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.models as models\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class DisparityTrainer(object):\n",
    "    def __init__(\n",
    "        self,\n",
    "        lr: float,\n",
    "        device: str,\n",
    "        trainlist: str,\n",
    "        vallist: str,\n",
    "        datapath: str,\n",
    "        batch_size: int,\n",
    "        maxdisp: int,\n",
    "        criterion: Callable,\n",
    "        pretrain: Optional[str]=None,\n",
    "        num_workers=4\n",
    "    ):\n",
    "        super(DisparityTrainer, self).__init__()\n",
    "        self.lr = lr\n",
    "        self.current_lr = lr\n",
    "        self.device = device\n",
    "        self.trainlist = trainlist\n",
    "        self.vallist = vallist\n",
    "        self.datapath = datapath\n",
    "        self.batch_size = batch_size\n",
    "        self.pretrain = pretrain\n",
    "        self.maxdisp = maxdisp\n",
    "        self.num_workers = num_workers\n",
    "        self.criterion = criterion\n",
    "        self.epe = EPE\n",
    "\n",
    "        self.initialize()\n",
    "\n",
    "    def _prepare_dataset(self):\n",
    "        train_dataset = IRSDataset(txt_file=self.trainlist, root_dir=self.datapath, phase='train', load_norm=False)\n",
    "        test_dataset = IRSDataset(txt_file=self.vallist, root_dir=self.datapath, phase='test', load_norm=False)\n",
    "        self.img_size = train_dataset.get_img_size()\n",
    "        self.scale_size = train_dataset.get_scale_size()\n",
    "        self.focal_length = train_dataset.get_focal_length()\n",
    "\n",
    "        self.train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "\n",
    "        self.test_loader = DataLoader(\n",
    "            test_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "        self.num_batches_per_epoch = len(self.train_loader)\n",
    "\n",
    "    def _build_net(self):\n",
    "        self.net = DispNetC(batchNorm=False, input_channel=3, maxdisp=30).to(self.device)\n",
    "\n",
    "        if self.pretrain is not None:\n",
    "            assert Path(self.pretrain).exists(), f\"{self.pretrain} does not exist\"\n",
    "            model_data = torch.load(self.pretrain)\n",
    "            assert \"model\" in model_data.keys(), f\"{self.pretrain} does not contain model\"\n",
    "            self.net.load_state_dict(model_data[\"model\"])\n",
    "\n",
    "    def _build_optimizer(self):\n",
    "        beta = 0.999\n",
    "        momentum = 0.9\n",
    "        self.optimizer = torch.optim.Adam(\n",
    "            filter(lambda p: p.requires_grad, self.net.parameters()),\n",
    "            self.lr,\n",
    "            betas=(momentum, beta),\n",
    "            amsgrad=True,\n",
    "        )\n",
    "\n",
    "    def initialize(self):\n",
    "        self._prepare_dataset()\n",
    "        self._build_net()\n",
    "        self._build_optimizer()\n",
    "\n",
    "    def adjust_learning_rate(self, epoch):\n",
    "        cur_lr = self.lr / (2 ** (epoch // 10))\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group[\"lr\"] = cur_lr\n",
    "        self.current_lr = cur_lr\n",
    "        return cur_lr\n",
    "\n",
    "    def train_one_epoch(self, epoch):\n",
    "        batch_time = AverageMeter()\n",
    "        data_time = AverageMeter()\n",
    "        losses = AverageMeter()\n",
    "        EPEs = AverageMeter()\n",
    "        # switch to train mode\n",
    "        self.net.train()\n",
    "        end = time.time()\n",
    "        cur_lr = self.adjust_learning_rate(epoch)\n",
    "        logger.info(\"learning rate of epoch %d: %f.\" % (epoch, cur_lr))\n",
    "\n",
    "        for i_batch, sample_batched in enumerate(self.train_loader):\n",
    "\n",
    "            left_input = sample_batched[\"img_left\"].to(self.device)\n",
    "            right_input = sample_batched[\"img_right\"].to(self.device)\n",
    "            input = torch.cat((left_input, right_input), 1)\n",
    "            target_disp = sample_batched[\"gt_disp\"].to(self.device)\n",
    "            data_time.update(time.time() - end)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            disps = self.net(input)\n",
    "            loss = self.criterion(left_input, right_input, disps[0])\n",
    "            epe = self.epe(disps[0], target_disp)\n",
    "            # record loss and EPE\n",
    "            losses.update(loss.data.item(), input.size(0))\n",
    "            EPEs.update(epe.data.item(), input.size(0))\n",
    "            # compute gradient and do SGD step\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            if i_batch % 10 == 0:\n",
    "                logger.info(\n",
    "                    \"Epoch: [{0}][{1}/{2}]\\t\"\n",
    "                    \"Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t\"\n",
    "                    \"Data {data_time.val:.3f} ({data_time.avg:.3f})\\t\"\n",
    "                    \"Loss {loss.val:.3f} ({loss.avg:.3f})\\t\"\n",
    "                    \"EPE {EPE.val:.3f} ({EPE.avg:.3f})\\t\".format(\n",
    "                        epoch,\n",
    "                        i_batch,\n",
    "                        self.num_batches_per_epoch,\n",
    "                        batch_time=batch_time,\n",
    "                        data_time=data_time,\n",
    "                        loss=losses,\n",
    "                        EPE=EPEs,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        return losses.avg, EPEs.avg\n",
    "\n",
    "    def validate(self):\n",
    "        batch_time = AverageMeter()\n",
    "        EPEs = AverageMeter()\n",
    "        # switch to evaluate mode\n",
    "        end = time.time()\n",
    "        self.net.eval()\n",
    "        for i, sample_batched in enumerate(self.test_loader):\n",
    "\n",
    "            left_input = sample_batched[\"img_left\"].to(self.device)\n",
    "            right_input = sample_batched[\"img_right\"].to(self.device)\n",
    "            left_input = F.interpolate(left_input, self.scale_size, mode=\"bilinear\")\n",
    "            right_input = F.interpolate(right_input, self.scale_size, mode=\"bilinear\")\n",
    "\n",
    "            input = torch.cat((left_input, right_input), 1)\n",
    "\n",
    "            target_disp = sample_batched[\"gt_disp\"].to(self.device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                disp = self.net(input)[0]\n",
    "\n",
    "            # upsampling the predicted disparity map\n",
    "            disp = nn.Upsample(size=target_disp.shape[2:], mode='bilinear')(disp)\n",
    "            epe = self.epe(disp, target_disp)\n",
    "\n",
    "            # record loss and EPE\n",
    "            EPEs.update(epe.data.item(), input.size(0))\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            logger.info(\n",
    "                \"Test: [{0}/{1}]\\t Time {2}\\t EPE {3}\".format(\n",
    "                    i,\n",
    "                    len(self.test_loader),\n",
    "                    batch_time.val,\n",
    "                    EPEs.val,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        logger.info(\" * EPE {:.3f}\".format(EPEs.avg))\n",
    "        return EPEs.avg\n",
    "\n",
    "    def get_model(self):\n",
    "        return self.net.state_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_left(im_l: torch.Tensor, im_r: torch.Tensor, disp: torch.Tensor):\n",
    "    '''\n",
    "    im_l, im_r: [N, C(3), H, W]\n",
    "    disp: [N, C(1), H, W]\n",
    "    '''\n",
    "    N, _, H, W = disp.shape\n",
    "    x_base = torch.linspace(0, 1, W).repeat(N, 1,\n",
    "                H, 1).type_as(im_l)\n",
    "    y_base = torch.linspace(0, 1, H).repeat(N, 1,\n",
    "                W, 1).transpose(2, 3).type_as(im_l)\n",
    "    x_query = x_base - (disp / W)\n",
    "    # [1, H, W]\n",
    "    valid_mask = x_query >= 0\n",
    "    flow_field = 2 * torch.stack((x_query, y_base), dim=4) - 1\n",
    "    est_l = F.grid_sample(im_r, flow_field.squeeze(), mode='bilinear', padding_mode='zeros')\n",
    "    return torch.where(valid_mask, est_l, im_l)\n",
    "\n",
    "class PerceptualLoss(nn.Module):\n",
    "    L1 = 4\n",
    "    L2 = 9\n",
    "    L3 = 16\n",
    "    L4 = 30\n",
    "\n",
    "    def __init__(self, layers: List[int], device: str):\n",
    "        super(PerceptualLoss, self).__init__()\n",
    "        self.layers = layers\n",
    "        vgg16 = models.vgg16(pretrained=True)\n",
    "        self.feat_seq = list(vgg16.named_children())[0][1].to(device)\n",
    "\n",
    "    def _compute_l1_diff(self, inputs: torch.Tensor, targets: torch.Tensor):\n",
    "        '''\n",
    "        Returns [N, M, H, W] where M is len(self.layers)\n",
    "        '''\n",
    "        diffs = []\n",
    "        H, W = inputs.shape[2:]\n",
    "        up = torch.nn.Upsample(size=(H,W))\n",
    "        for layer in self.layers:\n",
    "            input_map = self.feat_seq[0:layer](inputs)\n",
    "            target_map = self.feat_seq[0:layer](targets)\n",
    "            diffs.append(up(torch.norm(input_map - target_map, p=1, dim=1).unsqueeze(1)).squeeze(1))\n",
    "        return torch.stack(diffs, dim=1)\n",
    "\n",
    "    def _compare_images(self, est: torch.Tensor, gt: torch.Tensor):\n",
    "        '''\n",
    "        est, gt: [N, C(3), H, W]\n",
    "        '''\n",
    "        return torch.prod(self._compute_l1_diff(est, gt), dim=1).mean()\n",
    "\n",
    "    def forward(self, im_l: torch.Tensor, im_r: torch.Tensor, disp: torch.Tensor):\n",
    "        '''\n",
    "        im_l, im_r: [N, C(3), H, W]\n",
    "        disp: [N, C(1), H, W]\n",
    "        '''\n",
    "        return self._compare_images(estimate_left(im_l, im_r, disp), im_l)\n",
    "\n",
    "class L1Loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(L1Loss, self).__init__()\n",
    "    \n",
    "    def forward(self, im_l: torch.Tensor, im_r: torch.Tensor, disp: torch.Tensor):\n",
    "        est_l = estimate_left(im_l, im_r, disp)\n",
    "        return nn.L1Loss()(est_l, im_l)\n",
    "\n",
    "class SSLCriterion(nn.Module):\n",
    "    def __init__(self, modules: nn.ModuleList):\n",
    "        super(SSLCriterion, self).__init__()\n",
    "        self.modules = modules\n",
    "\n",
    "    def forward(self, im_l: torch.Tensor, im_r: torch.Tensor, disp: torch.Tensor):\n",
    "        '''\n",
    "        im_l, im_r: [N, C(3), H, W]\n",
    "        disp: [N, C(1), H, W]\n",
    "        '''\n",
    "        return sum([mod(im_l, im_r, disp) for mod in self.modules])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/project/IRS/networks/DispNetC.py:117: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  kaiming_normal(m.weight.data)\n",
      "2022-04-18 20:20:42,940 [1914307141.py:118] INFO learning rate of epoch 0: 0.000200.\n",
      "/project/IRS/utils/preprocess.py:154: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)\n",
      "  gs[0].mul_(0.299).add_(0.587, gs[1]).add_(0.114, gs[2])\n",
      "/project/IRS/utils/preprocess.py:154: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)\n",
      "  gs[0].mul_(0.299).add_(0.587, gs[1]).add_(0.114, gs[2])\n",
      "/project/IRS/utils/preprocess.py:154: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)\n",
      "  gs[0].mul_(0.299).add_(0.587, gs[1]).add_(0.114, gs[2])\n",
      "/project/IRS/utils/preprocess.py:154: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)\n",
      "  gs[0].mul_(0.299).add_(0.587, gs[1]).add_(0.114, gs[2])\n",
      "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:4003: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "2022-04-18 20:20:46,018 [1914307141.py:145] INFO Epoch: [0][0/1171]\tTime 3.078 (3.078)\tData 2.303 (2.303)\tLoss 0.353 (0.353)\tEPE 14.296 (14.296)\t\n",
      "2022-04-18 20:20:52,506 [1914307141.py:145] INFO Epoch: [0][10/1171]\tTime 0.654 (0.870)\tData 0.015 (0.224)\tLoss 0.268 (0.331)\tEPE 11.408 (12.561)\t\n",
      "2022-04-18 20:20:58,974 [1914307141.py:145] INFO Epoch: [0][20/1171]\tTime 0.656 (0.763)\tData 0.016 (0.125)\tLoss 0.303 (0.335)\tEPE 15.898 (12.498)\t\n",
      "2022-04-18 20:21:05,487 [1914307141.py:145] INFO Epoch: [0][30/1171]\tTime 0.600 (0.727)\tData 0.015 (0.089)\tLoss 0.329 (0.339)\tEPE 12.838 (12.639)\t\n",
      "2022-04-18 20:21:11,905 [1914307141.py:145] INFO Epoch: [0][40/1171]\tTime 0.677 (0.706)\tData 0.016 (0.071)\tLoss 0.299 (0.337)\tEPE 14.346 (12.184)\t\n",
      "2022-04-18 20:21:18,241 [1914307141.py:145] INFO Epoch: [0][50/1171]\tTime 0.668 (0.692)\tData 0.015 (0.060)\tLoss 0.287 (0.336)\tEPE 13.156 (12.084)\t\n",
      "2022-04-18 20:21:24,679 [1914307141.py:145] INFO Epoch: [0][60/1171]\tTime 0.604 (0.684)\tData 0.015 (0.053)\tLoss 0.289 (0.338)\tEPE 10.017 (11.912)\t\n",
      "2022-04-18 20:21:31,224 [1914307141.py:145] INFO Epoch: [0][70/1171]\tTime 0.659 (0.680)\tData 0.015 (0.048)\tLoss 0.286 (0.330)\tEPE 15.312 (12.033)\t\n",
      "2022-04-18 20:21:37,678 [1914307141.py:145] INFO Epoch: [0][80/1171]\tTime 0.636 (0.676)\tData 0.014 (0.044)\tLoss 0.288 (0.330)\tEPE 9.592 (11.906)\t\n",
      "2022-04-18 20:21:44,195 [1914307141.py:145] INFO Epoch: [0][90/1171]\tTime 0.654 (0.673)\tData 0.016 (0.041)\tLoss 0.356 (0.330)\tEPE 16.957 (11.814)\t\n",
      "2022-04-18 20:21:50,706 [1914307141.py:145] INFO Epoch: [0][100/1171]\tTime 0.657 (0.671)\tData 0.016 (0.039)\tLoss 0.311 (0.328)\tEPE 12.179 (11.914)\t\n",
      "2022-04-18 20:21:57,202 [1914307141.py:145] INFO Epoch: [0][110/1171]\tTime 0.635 (0.669)\tData 0.014 (0.037)\tLoss 0.337 (0.324)\tEPE 9.361 (11.792)\t\n",
      "2022-04-18 20:22:03,614 [1914307141.py:145] INFO Epoch: [0][120/1171]\tTime 0.604 (0.667)\tData 0.014 (0.035)\tLoss 0.345 (0.323)\tEPE 14.889 (11.727)\t\n",
      "2022-04-18 20:22:10,088 [1914307141.py:145] INFO Epoch: [0][130/1171]\tTime 0.662 (0.665)\tData 0.015 (0.034)\tLoss 0.232 (0.319)\tEPE 7.880 (11.584)\t\n",
      "2022-04-18 20:22:16,470 [1914307141.py:145] INFO Epoch: [0][140/1171]\tTime 0.646 (0.663)\tData 0.015 (0.032)\tLoss 0.349 (0.318)\tEPE 12.688 (11.606)\t\n",
      "2022-04-18 20:22:22,976 [1914307141.py:145] INFO Epoch: [0][150/1171]\tTime 0.606 (0.662)\tData 0.015 (0.031)\tLoss 0.345 (0.316)\tEPE 12.883 (11.528)\t\n",
      "2022-04-18 20:22:29,463 [1914307141.py:145] INFO Epoch: [0][160/1171]\tTime 0.636 (0.662)\tData 0.020 (0.030)\tLoss 0.261 (0.315)\tEPE 14.400 (11.498)\t\n",
      "2022-04-18 20:22:36,116 [1914307141.py:145] INFO Epoch: [0][170/1171]\tTime 0.651 (0.662)\tData 0.014 (0.029)\tLoss 0.283 (0.313)\tEPE 13.348 (11.514)\t\n",
      "2022-04-18 20:22:42,618 [1914307141.py:145] INFO Epoch: [0][180/1171]\tTime 0.680 (0.661)\tData 0.015 (0.029)\tLoss 0.286 (0.311)\tEPE 11.106 (11.468)\t\n",
      "2022-04-18 20:22:49,134 [1914307141.py:145] INFO Epoch: [0][190/1171]\tTime 0.678 (0.661)\tData 0.014 (0.028)\tLoss 0.300 (0.310)\tEPE 11.384 (11.465)\t\n",
      "2022-04-18 20:22:55,492 [1914307141.py:145] INFO Epoch: [0][200/1171]\tTime 0.650 (0.659)\tData 0.015 (0.027)\tLoss 0.329 (0.309)\tEPE 7.580 (11.368)\t\n",
      "2022-04-18 20:23:01,963 [1914307141.py:145] INFO Epoch: [0][210/1171]\tTime 0.606 (0.659)\tData 0.014 (0.027)\tLoss 0.275 (0.307)\tEPE 9.192 (11.329)\t\n",
      "2022-04-18 20:23:08,400 [1914307141.py:145] INFO Epoch: [0][220/1171]\tTime 0.677 (0.658)\tData 0.032 (0.027)\tLoss 0.256 (0.307)\tEPE 9.162 (11.285)\t\n",
      "2022-04-18 20:23:14,928 [1914307141.py:145] INFO Epoch: [0][230/1171]\tTime 0.686 (0.658)\tData 0.019 (0.026)\tLoss 0.282 (0.306)\tEPE 10.660 (11.257)\t\n",
      "2022-04-18 20:23:21,419 [1914307141.py:145] INFO Epoch: [0][240/1171]\tTime 0.643 (0.658)\tData 0.018 (0.026)\tLoss 0.315 (0.305)\tEPE 7.214 (11.226)\t\n",
      "2022-04-18 20:23:27,969 [1914307141.py:145] INFO Epoch: [0][250/1171]\tTime 0.608 (0.657)\tData 0.017 (0.025)\tLoss 0.278 (0.304)\tEPE 10.160 (11.254)\t\n",
      "2022-04-18 20:23:34,587 [1914307141.py:145] INFO Epoch: [0][260/1171]\tTime 0.666 (0.658)\tData 0.043 (0.025)\tLoss 0.283 (0.304)\tEPE 6.878 (11.197)\t\n",
      "2022-04-18 20:23:40,998 [1914307141.py:145] INFO Epoch: [0][270/1171]\tTime 0.603 (0.657)\tData 0.015 (0.025)\tLoss 0.371 (0.304)\tEPE 15.294 (11.261)\t\n",
      "2022-04-18 20:23:47,441 [1914307141.py:145] INFO Epoch: [0][280/1171]\tTime 0.610 (0.657)\tData 0.017 (0.024)\tLoss 0.305 (0.304)\tEPE 8.707 (11.253)\t\n",
      "2022-04-18 20:23:53,887 [1914307141.py:145] INFO Epoch: [0][290/1171]\tTime 0.639 (0.656)\tData 0.018 (0.024)\tLoss 0.315 (0.303)\tEPE 13.073 (11.268)\t\n",
      "2022-04-18 20:24:00,430 [1914307141.py:145] INFO Epoch: [0][300/1171]\tTime 0.682 (0.656)\tData 0.015 (0.024)\tLoss 0.347 (0.302)\tEPE 12.363 (11.203)\t\n",
      "2022-04-18 20:24:06,920 [1914307141.py:145] INFO Epoch: [0][310/1171]\tTime 0.652 (0.656)\tData 0.015 (0.024)\tLoss 0.322 (0.300)\tEPE 17.493 (11.167)\t\n",
      "2022-04-18 20:24:13,442 [1914307141.py:145] INFO Epoch: [0][320/1171]\tTime 0.656 (0.656)\tData 0.016 (0.023)\tLoss 0.259 (0.299)\tEPE 17.302 (11.185)\t\n",
      "2022-04-18 20:24:19,967 [1914307141.py:145] INFO Epoch: [0][330/1171]\tTime 0.654 (0.656)\tData 0.016 (0.023)\tLoss 0.247 (0.298)\tEPE 11.711 (11.213)\t\n",
      "2022-04-18 20:24:26,481 [1914307141.py:145] INFO Epoch: [0][340/1171]\tTime 0.665 (0.656)\tData 0.016 (0.023)\tLoss 0.258 (0.298)\tEPE 14.948 (11.195)\t\n",
      "2022-04-18 20:24:33,000 [1914307141.py:145] INFO Epoch: [0][350/1171]\tTime 0.675 (0.655)\tData 0.015 (0.023)\tLoss 0.274 (0.297)\tEPE 11.561 (11.208)\t\n",
      "2022-04-18 20:24:39,367 [1914307141.py:145] INFO Epoch: [0][360/1171]\tTime 0.655 (0.655)\tData 0.015 (0.023)\tLoss 0.325 (0.297)\tEPE 10.295 (11.195)\t\n",
      "2022-04-18 20:24:45,836 [1914307141.py:145] INFO Epoch: [0][370/1171]\tTime 0.610 (0.655)\tData 0.017 (0.023)\tLoss 0.213 (0.296)\tEPE 10.451 (11.161)\t\n",
      "2022-04-18 20:24:52,427 [1914307141.py:145] INFO Epoch: [0][380/1171]\tTime 0.679 (0.655)\tData 0.014 (0.022)\tLoss 0.256 (0.295)\tEPE 13.000 (11.171)\t\n",
      "2022-04-18 20:24:58,913 [1914307141.py:145] INFO Epoch: [0][390/1171]\tTime 0.654 (0.655)\tData 0.015 (0.022)\tLoss 0.204 (0.294)\tEPE 7.501 (11.154)\t\n"
     ]
    }
   ],
   "source": [
    "# Main training loop\n",
    "EXP_NAME = 'L1'\n",
    "CKPT_DIR = Path(f'ckpts/{EXP_NAME}')\n",
    "CKPT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "RESULTS_DIR = Path(f'results/{EXP_NAME}')\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "PRETRAIN_CHECKPOINT = None\n",
    "\n",
    "LR = 0.0002\n",
    "DEVICE = 'cuda:1'\n",
    "DATA_DIR = \"/data\"\n",
    "TRAIN_LIST = 'IRS/lists/Restaurant_TRAIN.list'\n",
    "TEST_METAL_LIST = 'IRS/lists/IRS_restaurant_metal_test.list'\n",
    "BATCH_SIZE = 16\n",
    "MAX_DISP = 192\n",
    "NUM_WORKERS = 4\n",
    "EPOCHS = 30\n",
    "\n",
    "\n",
    "# per_loss = PerceptualLoss([PerceptualLoss.L1], DEVICE)\n",
    "l1_loss = L1Loss()\n",
    "criterion = SSLCriterion([l1_loss])\n",
    "trainer = DisparityTrainer(\n",
    "    lr=LR,\n",
    "    device=DEVICE,\n",
    "    trainlist=TRAIN_LIST,\n",
    "    vallist=TEST_METAL_LIST,\n",
    "    datapath=DATA_DIR,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    maxdisp=MAX_DISP,\n",
    "    criterion=criterion,\n",
    "    pretrain=PRETRAIN_CHECKPOINT,\n",
    "    num_workers=NUM_WORKERS\n",
    ")\n",
    "\n",
    "\n",
    "best_EPE = 10000\n",
    "train_losses, train_EPEs, val_EPEs = [], [], []\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_EPE = trainer.train_one_epoch(epoch)\n",
    "    val_EPE = trainer.validate()\n",
    "    train_losses.append(train_loss)\n",
    "    train_EPEs.append(train_EPE)\n",
    "    val_EPEs.append(val_EPE)\n",
    "    ckpt = {\n",
    "        'epoch': epoch,\n",
    "        'epe': val_EPE,\n",
    "        'model': trainer.get_model(),\n",
    "    }\n",
    "    if val_EPE < best_EPE:\n",
    "        save_file = f\"dispnet_epoch_{epoch}_BEST\"\n",
    "    else:\n",
    "        save_file = f\"dispnet_epoch_{epoch}\"\n",
    "    best_EPE = min(best_EPE, val_EPE)\n",
    "    torch.save(ckpt, CKPT_DIR / save_file)\n",
    "    np.save(RESULTS_DIR / \"train_losses\", np.array(train_losses))\n",
    "    np.save(RESULTS_DIR / \"train_EPEs\", np.array(train_EPEs))\n",
    "    np.save(RESULTS_DIR / \"val_EPEs\", np.array(val_EPEs))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
