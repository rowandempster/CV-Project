{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/project/IRS/utils/preprocess.py:154: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)\n",
      "  gs[0].mul_(0.299).add_(0.587, gs[1]).add_(0.114, gs[2])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 384, 768])\n"
     ]
    }
   ],
   "source": [
    "# Load the IRS/lists/IRS_metal_test.list file into the IRSDataset\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from IRS.dataloader.IRSLoader import IRSDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from IRS.networks.DispNetC import DispNetC\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.models as models\n",
    "from typing import List\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "DEVICE = 'cuda:0'\n",
    "DATA_DIR = \"/data\"\n",
    "NEW_LIST = 'IRS/lists/IRS_restaurant_metal_test.list'\n",
    "BATCH_SIZE = 2\n",
    "NUM_WORKERS = 1\n",
    "dataset = IRSDataset(txt_file=NEW_LIST, root_dir=DATA_DIR, phase='train')\n",
    "train_loader = DataLoader(dataset, batch_size = BATCH_SIZE, \\\n",
    "                                shuffle = True, num_workers = NUM_WORKERS, \\\n",
    "                                pin_memory = True)\n",
    "net = DispNetC(batchNorm=False, input_channel=3, maxdisp=30).to(DEVICE)\n",
    "batch = next(iter(train_loader))\n",
    "input = torch.cat((batch['img_left'].to(DEVICE), batch['img_right'].to(DEVICE)), dim=1)\n",
    "disp = net(input)\n",
    "print(disp[0].shape)\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import time\n",
    "from typing import Callable, Optional\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from pathlib import Path\n",
    "\n",
    "from IRS.dataloader.IRSLoader import IRSDataset\n",
    "from IRS.utils.AverageMeter import AverageMeter\n",
    "from IRS.utils.common import logger\n",
    "from IRS.losses.multiscaleloss import EPE\n",
    "from IRS.dataloader.IRSLoader import IRSDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from IRS.networks.DispNetC import DispNetC\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.models as models\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class DisparityTrainer(object):\n",
    "    def __init__(\n",
    "        self,\n",
    "        lr: float,\n",
    "        device: str,\n",
    "        trainlist: str,\n",
    "        vallist: str,\n",
    "        datapath: str,\n",
    "        batch_size: int,\n",
    "        maxdisp: int,\n",
    "        criterion: Callable,\n",
    "        pretrain: Optional[str]=None,\n",
    "        num_workers=4\n",
    "    ):\n",
    "        super(DisparityTrainer, self).__init__()\n",
    "        self.lr = lr\n",
    "        self.current_lr = lr\n",
    "        self.device = device\n",
    "        self.trainlist = trainlist\n",
    "        self.vallist = vallist\n",
    "        self.datapath = datapath\n",
    "        self.batch_size = batch_size\n",
    "        self.pretrain = pretrain\n",
    "        self.maxdisp = maxdisp\n",
    "        self.num_workers = num_workers\n",
    "        self.criterion = criterion\n",
    "        self.epe = EPE\n",
    "\n",
    "        self.initialize()\n",
    "\n",
    "    def _prepare_dataset(self):\n",
    "        train_dataset = IRSDataset(txt_file=self.trainlist, root_dir=self.datapath, phase='train', load_norm=False)\n",
    "        test_dataset = IRSDataset(txt_file=self.vallist, root_dir=self.datapath, phase='test', load_norm=False)\n",
    "        self.img_size = train_dataset.get_img_size()\n",
    "        self.scale_size = train_dataset.get_scale_size()\n",
    "        self.focal_length = train_dataset.get_focal_length()\n",
    "\n",
    "        self.train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "\n",
    "        self.test_loader = DataLoader(\n",
    "            test_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "        self.num_batches_per_epoch = len(self.train_loader)\n",
    "\n",
    "    def _build_net(self):\n",
    "        self.net = DispNetC(batchNorm=False, input_channel=3, maxdisp=30).to(self.device)\n",
    "\n",
    "        if self.pretrain is not None:\n",
    "            assert Path(self.pretrain).exists(), f\"{self.pretrain} does not exist\"\n",
    "            model_data = torch.load(self.pretrain)\n",
    "            logger.info(\"Load pretrain model: %s\", self.pretrain)\n",
    "            if \"state_dict\" in model_data.keys():\n",
    "                self.net.load_state_dict(model_data[\"state_dict\"])\n",
    "            elif \"model\" in model_data.keys():\n",
    "                self.net.load_state_dict(model_data[\"model\"])\n",
    "            else:\n",
    "                self.net.load_state_dict(model_data)\n",
    "\n",
    "    def _build_optimizer(self):\n",
    "        beta = 0.999\n",
    "        momentum = 0.9\n",
    "        self.optimizer = torch.optim.Adam(\n",
    "            filter(lambda p: p.requires_grad, self.net.parameters()),\n",
    "            self.lr,\n",
    "            betas=(momentum, beta),\n",
    "            amsgrad=True,\n",
    "        )\n",
    "\n",
    "    def initialize(self):\n",
    "        self._prepare_dataset()\n",
    "        self._build_net()\n",
    "        self._build_optimizer()\n",
    "\n",
    "    def adjust_learning_rate(self, epoch):\n",
    "        cur_lr = self.lr / (2 ** (epoch // 10))\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group[\"lr\"] = cur_lr\n",
    "        self.current_lr = cur_lr\n",
    "        return cur_lr\n",
    "\n",
    "    def train_one_epoch(self, epoch):\n",
    "        batch_time = AverageMeter()\n",
    "        data_time = AverageMeter()\n",
    "        losses = AverageMeter()\n",
    "        EPEs = AverageMeter()\n",
    "        # switch to train mode\n",
    "        self.net.train()\n",
    "        end = time.time()\n",
    "        cur_lr = self.adjust_learning_rate(epoch)\n",
    "        logger.info(\"learning rate of epoch %d: %f.\" % (epoch, cur_lr))\n",
    "\n",
    "        for i_batch, sample_batched in enumerate(self.train_loader):\n",
    "\n",
    "            left_input = sample_batched[\"img_left\"].to(self.device)\n",
    "            right_input = sample_batched[\"img_right\"].to(self.device)\n",
    "            input = torch.cat((left_input, right_input), 1)\n",
    "            target_disp = sample_batched[\"gt_disp\"].to(self.device)\n",
    "            data_time.update(time.time() - end)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            disps = self.net(input)\n",
    "            loss = self.criterion(disps, left_input, right_input)\n",
    "            epe = self.epe(disps[0], target_disp)\n",
    "            # record loss and EPE\n",
    "            losses.update(loss.data.item(), input.size(0))\n",
    "            EPEs.update(epe.data.item(), input.size(0))\n",
    "            # compute gradient and do SGD step\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            if i_batch % 10 == 0:\n",
    "                logger.info(\n",
    "                    \"Epoch: [{0}][{1}/{2}]\\t\"\n",
    "                    \"Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t\"\n",
    "                    \"Data {data_time.val:.3f} ({data_time.avg:.3f})\\t\"\n",
    "                    \"Loss {loss.val:.3f} ({loss.avg:.3f})\\t\"\n",
    "                    \"EPE {EPE.val:.3f} ({EPE.avg:.3f})\\t\".format(\n",
    "                        epoch,\n",
    "                        i_batch,\n",
    "                        self.num_batches_per_epoch,\n",
    "                        batch_time=batch_time,\n",
    "                        data_time=data_time,\n",
    "                        loss=losses,\n",
    "                        EPE=EPEs,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        return losses.avg, EPEs.avg\n",
    "\n",
    "    def validate(self):\n",
    "        batch_time = AverageMeter()\n",
    "        EPEs = AverageMeter()\n",
    "        # switch to evaluate mode\n",
    "        end = time.time()\n",
    "        self.net.eval()\n",
    "        for i, sample_batched in enumerate(self.test_loader):\n",
    "\n",
    "            left_input = sample_batched[\"img_left\"].to(self.device)\n",
    "            right_input = sample_batched[\"img_right\"].to(self.device)\n",
    "            left_input = F.interpolate(left_input, self.scale_size, mode=\"bilinear\")\n",
    "            right_input = F.interpolate(right_input, self.scale_size, mode=\"bilinear\")\n",
    "\n",
    "            input = torch.cat((left_input, right_input), 1)\n",
    "\n",
    "            target_disp = sample_batched[\"gt_disp\"].to(self.device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                disp = self.net(input)[0]\n",
    "\n",
    "            # upsampling the predicted disparity map\n",
    "            disp = nn.Upsample(size=target_disp.shape[2:], mode='bilinear')(disp)\n",
    "            epe = self.epe(disp, target_disp)\n",
    "\n",
    "            # record loss and EPE\n",
    "            EPEs.update(epe.data.item(), input.size(0))\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            logger.info(\n",
    "                \"Test: [{0}/{1}]\\t Time {2}\\t EPE {3}\".format(\n",
    "                    i,\n",
    "                    len(self.test_loader),\n",
    "                    batch_time.val,\n",
    "                    EPEs.val,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        logger.info(\" * EPE {:.3f}\".format(EPEs.avg))\n",
    "        return EPEs.avg\n",
    "\n",
    "    def get_model(self):\n",
    "        return self.net.state_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerceptualLoss(nn.Module):\n",
    "    L1 = 4\n",
    "    L2 = 9\n",
    "    L3 = 16\n",
    "    L4 = 30\n",
    "\n",
    "    def __init__(self, layers: List[int]):\n",
    "        super(PerceptualLoss, self).__init__()\n",
    "        self.layers = layers\n",
    "        vgg16 = models.vgg16(pretrained=True)\n",
    "        self.feat_seq = list(vgg16.named_children())[0][1]\n",
    "\n",
    "    def _compute_l1_diff(self, inputs: torch.Tensor, targets: torch.Tensor):\n",
    "        '''\n",
    "        Returns [N, M, H, W] where M is len(self.layers)\n",
    "        '''\n",
    "        diffs = []\n",
    "        H, W = inputs.shape[2:]\n",
    "        up = torch.nn.Upsample(size=(H,W))\n",
    "        for layer in self.layers:\n",
    "            input_map = self.feat_seq[0:layer](inputs)\n",
    "            target_map = self.feat_seq[0:layer](targets)\n",
    "            diffs.append(up(torch.norm(input_map - target_map, p=1, dim=1).unsqueeze(1)).squeeze(1))\n",
    "        return torch.stack(diffs, dim=1)\n",
    "\n",
    "    def _compare_images(self, est: torch.Tensor, gt: torch.Tensor):\n",
    "        '''\n",
    "        est, gt: [N, C(3), H, W]\n",
    "        '''\n",
    "        return torch.prod(self._compute_l1_diff(est, gt), dim=1).mean()\n",
    "\n",
    "    def _estimate_left(self, im_l: torch.Tensor, im_r: torch.Tensor, disp: torch.Tensor):\n",
    "        '''\n",
    "        im_l, im_r: [N, C(3), H, W]\n",
    "        disp: [N, C(1), H, W]\n",
    "        '''\n",
    "        N, _, H, W = disp.shape\n",
    "        x_base = torch.linspace(0, W-1, W).repeat(N, H, 1).to(im_r.device)\n",
    "        # [1, H, W]\n",
    "        x_query = (x_base - disp).round().long()\n",
    "        valid_mask = x_query >= 0\n",
    "        return torch.where(valid_mask, im_r.gather(3, x_query.clip(0, 959).expand_as(im_r)), im_l)\n",
    "\n",
    "    def forward(self, im_l: torch.Tensor, im_r: torch.Tensor, disp: torch.Tensor):\n",
    "        '''\n",
    "        im_l, im_r: [N, C(3), H, W]\n",
    "        disp: [N, C(1), H, W]\n",
    "        '''\n",
    "        return self._compare_images(self._estimate_left(im_l, im_r, disp), im_l)\n",
    "\n",
    "\n",
    "class SSLCriterion(nn.Module):\n",
    "    def __init__(self, modules: nn.ModuleList):\n",
    "        super(SSLCriterion, self).__init__()\n",
    "        self.modules = modules\n",
    "\n",
    "    def forward(self, im_l: torch.Tensor, im_r: torch.Tensor, disp: torch.Tensor):\n",
    "        '''\n",
    "        im_l, im_r: [N, C(3), H, W]\n",
    "        disp: [N, C(1), H, W]\n",
    "        '''\n",
    "        return sum([mod(im_l, im_r, disp) for mod in self.modules])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/project/IRS/networks/DispNetC.py:117: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  kaiming_normal(m.weight.data)\n",
      "2022-04-17 18:10:05,712 [4247811189.py:123] INFO learning rate of epoch 0: 0.000200.\n"
     ]
    }
   ],
   "source": [
    "# Main training loop\n",
    "LR = 0.0002\n",
    "DEVICE = 'cuda:0'\n",
    "DATA_DIR = \"/data\"\n",
    "TRAIN_LIST = 'IRS/lists/Restaurant_TRAIN.list'\n",
    "TEST_METAL_LIST = 'IRS/lists/IRS_restaurant_metal_test.list'\n",
    "PRETRAIN_CHECKPOINT = None\n",
    "BATCH_SIZE = 8\n",
    "MAX_DISP = 200\n",
    "NUM_WORKERS = 4\n",
    "EPOCHS = 1\n",
    "\n",
    "per_loss = PerceptualLoss([PerceptualLoss.L1])\n",
    "criterion = SSLCriterion([per_loss])\n",
    "trainer = DisparityTrainer(\n",
    "    lr=LR,\n",
    "    device=DEVICE,\n",
    "    trainlist=TRAIN_LIST,\n",
    "    vallist=TEST_METAL_LIST,\n",
    "    datapath=DATA_DIR,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    maxdisp=MAX_DISP,\n",
    "    criterion=criterion,\n",
    "    pretrain=PRETRAIN_CHECKPOINT,\n",
    "    num_workers=NUM_WORKERS\n",
    ")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    trainer.train_one_epoch(epoch)\n",
    "    trainer.validate()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
